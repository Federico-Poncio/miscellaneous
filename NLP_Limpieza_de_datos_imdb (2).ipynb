{"cells":[{"cell_type":"markdown","metadata":{"id":"JtLWSLucw60H"},"source":["# NLP - Limpeza de datos de comentarios imdb"]},{"cell_type":"markdown","metadata":{"id":"Qgd65I6uw60M"},"source":["### Proximos pasos:\n","* Normalizar pronombres: Reemplazar entidades que refieran a personas por MALE y FEMALE (personajes, actores y pronombres) --> https://www.researchgate.net/profile/Edgar_Altszyler/publication/331304421_Half_a_Century_of_Stereotyping_Associations_Between_Gender_and_Intellectual_Ability_in_Films/links/5cbcbb064585156cd7a8c3a5/Half-a-Century-of-Stereotyping-Associations-Between-Gender-and-Intellectual-Ability-in-Films.pdf\n","* Name Entity Recognition --> https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\n","* Sentiment analysis --> Text blob: https://textblob.readthedocs.io/en/dev/quickstart.html\n","* Empezar corriendo un NAIVE BAYES sin hacer ninguna variable a ver como da la prediccion. Eso nos va a dar un piso. https://textblob.readthedocs.io/en/dev/classifiers.html#classifiers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZQm8Ta1w60O"},"outputs":[],"source":["# Importo librerias\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.tokenize.toktok import ToktokTokenizer\n","#import nltk\n","#nltk.download('stopwords')\n","# ! pip install gensim\n","from gensim import corpora, models, similarities, matutils\n","from gensim.models.word2vec import Word2Vec\n","from gensim.models import KeyedVectors\n","from gensim.matutils import cossim\n","from gensim.models.phrases import Phrases, Phraser\n","#! pip install normalise\n","#for dependency in (\"brown\", \"names\", \"wordnet\", \"averaged_perceptron_tagger\", \"universal_tagset\"):\n","#    nltk.download(dependency)\n","from normalise import normalise\n","#! pip install textblob\n","from textblob import TextBlob\n","#! pip install emoji\n","import emoji\n","# imports\n","import unicodedata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jBo-eqUw60R","outputId":"500edf28-481b-4e34-c0f4-7e471062664b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>imdbid</th>\n","      <th>rev_title</th>\n","      <th>rev_content</th>\n","      <th>rating</th>\n","      <th>rev_link</th>\n","      <th>rev_username</th>\n","      <th>rev_date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>tt3907584</td>\n","      <td>A spectacular movie\\n</td>\n","      <td>The first act felt annoyingly pretentious, how...</td>\n","      <td>9</td>\n","      <td>/review/rw5556178/</td>\n","      <td>MR_Heraclius</td>\n","      <td>16 March 2020</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>tt3907584</td>\n","      <td>so....read this up.\\n</td>\n","      <td>I did not know this was made after a book. I d...</td>\n","      <td>9</td>\n","      <td>/review/rw5515902/</td>\n","      <td>suciulaurentiucristian</td>\n","      <td>28 February 2020</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>tt3907584</td>\n","      <td>they had so much but they gave too little\\n</td>\n","      <td>I had this feeling since they announced the mo...</td>\n","      <td>5</td>\n","      <td>/review/rw5515384/</td>\n","      <td>mermariam</td>\n","      <td>28 February 2020</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>tt3907584</td>\n","      <td>People who call him selfish have never experi...</td>\n","      <td>I saw some negative reviews not getting the lo...</td>\n","      <td>10</td>\n","      <td>/review/rw5525473/</td>\n","      <td>viktoriabr-21711</td>\n","      <td>4 March 2020</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>tt3907584</td>\n","      <td>Beautiful movie.\\n</td>\n","      <td>Randomly found this movie today and I only hav...</td>\n","      <td>10</td>\n","      <td>/review/rw5516559/</td>\n","      <td>aeo-90520</td>\n","      <td>29 February 2020</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0     imdbid                                          rev_title  \\\n","0           0  tt3907584                              A spectacular movie\\n   \n","1           1  tt3907584                              so....read this up.\\n   \n","2           2  tt3907584        they had so much but they gave too little\\n   \n","3           3  tt3907584   People who call him selfish have never experi...   \n","4           4  tt3907584                                 Beautiful movie.\\n   \n","\n","                                         rev_content  rating  \\\n","0  The first act felt annoyingly pretentious, how...       9   \n","1  I did not know this was made after a book. I d...       9   \n","2  I had this feeling since they announced the mo...       5   \n","3  I saw some negative reviews not getting the lo...      10   \n","4  Randomly found this movie today and I only hav...      10   \n","\n","             rev_link            rev_username          rev_date  \n","0  /review/rw5556178/            MR_Heraclius     16 March 2020  \n","1  /review/rw5515902/  suciulaurentiucristian  28 February 2020  \n","2  /review/rw5515384/               mermariam  28 February 2020  \n","3  /review/rw5525473/        viktoriabr-21711      4 March 2020  \n","4  /review/rw5516559/               aeo-90520  29 February 2020  "]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["#Cargo el dataset que contiene las reviews\n","df = pd.read_csv('C:/Users/tomas/Desktop/Maestria Data Mining/Text Mining/imdb_reviews.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"eVOFMopaw60T"},"source":["### Poner la fecha en formato YYYYMMDD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmdqIy96w60U"},"outputs":[],"source":["from dateutil import parser\n","df['rev_date']=df['rev_date'].apply(lambda x: parser.parse(x).strftime('%Y%m%d'))"]},{"cell_type":"markdown","metadata":{"id":"4bzZhdEIw60U"},"source":["### Creo 'df_prueba' (df reducido a 50 reviews) para hacer pruebas de forma mas comoda"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sq09TTW3w60V","outputId":"47ffc331-f455-4768-8d16-16c76c2c83ad"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>imdbid</th>\n","      <th>rev_title</th>\n","      <th>rev_content</th>\n","      <th>rating</th>\n","      <th>rev_link</th>\n","      <th>rev_username</th>\n","      <th>rev_date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>tt3907584</td>\n","      <td>A spectacular movie\\n</td>\n","      <td>The first act felt annoyingly pretentious, how...</td>\n","      <td>9</td>\n","      <td>/review/rw5556178/</td>\n","      <td>MR_Heraclius</td>\n","      <td>20200316</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>tt3907584</td>\n","      <td>so....read this up.\\n</td>\n","      <td>I did not know this was made after a book. I d...</td>\n","      <td>9</td>\n","      <td>/review/rw5515902/</td>\n","      <td>suciulaurentiucristian</td>\n","      <td>20200228</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>tt3907584</td>\n","      <td>they had so much but they gave too little\\n</td>\n","      <td>I had this feeling since they announced the mo...</td>\n","      <td>5</td>\n","      <td>/review/rw5515384/</td>\n","      <td>mermariam</td>\n","      <td>20200228</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>tt3907584</td>\n","      <td>People who call him selfish have never experi...</td>\n","      <td>I saw some negative reviews not getting the lo...</td>\n","      <td>10</td>\n","      <td>/review/rw5525473/</td>\n","      <td>viktoriabr-21711</td>\n","      <td>20200304</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>tt3907584</td>\n","      <td>Beautiful movie.\\n</td>\n","      <td>Randomly found this movie today and I only hav...</td>\n","      <td>10</td>\n","      <td>/review/rw5516559/</td>\n","      <td>aeo-90520</td>\n","      <td>20200229</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0     imdbid                                          rev_title  \\\n","0           0  tt3907584                              A spectacular movie\\n   \n","1           1  tt3907584                              so....read this up.\\n   \n","2           2  tt3907584        they had so much but they gave too little\\n   \n","3           3  tt3907584   People who call him selfish have never experi...   \n","4           4  tt3907584                                 Beautiful movie.\\n   \n","\n","                                         rev_content  rating  \\\n","0  The first act felt annoyingly pretentious, how...       9   \n","1  I did not know this was made after a book. I d...       9   \n","2  I had this feeling since they announced the mo...       5   \n","3  I saw some negative reviews not getting the lo...      10   \n","4  Randomly found this movie today and I only hav...      10   \n","\n","             rev_link            rev_username  rev_date  \n","0  /review/rw5556178/            MR_Heraclius  20200316  \n","1  /review/rw5515902/  suciulaurentiucristian  20200228  \n","2  /review/rw5515384/               mermariam  20200228  \n","3  /review/rw5525473/        viktoriabr-21711  20200304  \n","4  /review/rw5516559/               aeo-90520  20200229  "]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["# Creo df de prueba\n","df_prueba = df.loc[0:50,:]\n","df_prueba.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ny2IiGZw60W"},"outputs":[],"source":["# Renombro para que df sea df_prueba, asi en todos los analisis sigueintes se probara con ese subcojunto de datos\n","#df = df_prueba\n"]},{"cell_type":"markdown","metadata":{"id":"H8kLFdFHw60X"},"source":["## Cantidad de caracteres de las reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzFxL4sXw60X","outputId":"54d9f58b-53e8-4af7-d7ba-3bbcfc33a1f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of nulls in rev_content column: 0\n"]}],"source":["# Cantidad de review nulas \n","print(\"Number of nulls in rev_content column: {}\".format(df['rev_content'].isnull().sum()))"]},{"cell_type":"markdown","metadata":{"id":"-lllI78jw60Y"},"source":["Crear una nueva columna que cuente la cantidad de caracteres en las strigns de \"rev_content\".\n","Asi no solo se va a poder buscar reviews vacias, sino que luego se va a poder descartar por cantidad minima de caracteres dentro de una review"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4nLKuv4w60Z"},"outputs":[],"source":["df['rev_content_length']  = df['rev_content'].apply(lambda x: len(x))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"anUZNXfzw60Z","outputId":"52ad8911-7bd4-4340-ddc5-89c170dfe388"},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Hago un plot de la cantidad de caracteres que tiene cada review\n","import matplotlib.pyplot as plt\n","x = df['rev_content_length'] \n","plt.hist(x, bins = 300)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-fk1wOQw60a","outputId":"8a62df23-9bc0-4ac0-e7f1-5d0e01713cb7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>imdbid</th>\n","      <th>rev_title</th>\n","      <th>rev_content</th>\n","      <th>rating</th>\n","      <th>rev_link</th>\n","      <th>rev_username</th>\n","      <th>rev_date</th>\n","      <th>rev_content_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [Unnamed: 0, imdbid, rev_title, rev_content, rating, rev_link, rev_username, rev_date, rev_content_length]\n","Index: []"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["df[df['rev_content_length']<3]"]},{"cell_type":"markdown","metadata":{"id":"-96gzZI2w60a"},"source":["No hay instancias en el df que tengan menos de 3 caracteres como texto de review.\n","Sin embargo, esto si que nos deja para pensar cuanto es el numero minimo de caracteres para que no los descartemos. Hay reviews de 6 caracteres (e.g. \"Great!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeYRT2Skw60b","outputId":"e41a7ff6-9f3c-45b5-ea06-46d2b5cf6850"},"outputs":[{"data":{"text/plain":["rev_content_length\n","50      1\n","60      1\n","85      1\n","105     1\n","114     1\n","115     1\n","124     1\n","127     1\n","145     1\n","149     1\n","221     1\n","226     1\n","233     1\n","255     1\n","309     1\n","327     1\n","329     1\n","330     1\n","349     1\n","397     1\n","410     1\n","455     1\n","486     1\n","517     1\n","542     1\n","558     1\n","559     1\n","567     1\n","569     1\n","581     1\n","584     1\n","589     1\n","594     1\n","621     1\n","622     1\n","672     1\n","754     1\n","822     1\n","847     1\n","862     1\n","956     1\n","974     1\n","1371    1\n","1403    1\n","1452    1\n","1502    1\n","1506    1\n","1591    1\n","2217    1\n","2663    1\n","2788    1\n","dtype: int64"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["contador_de_caracteres = df.groupby(['rev_content_length']).size()\n","contador_de_caracteres"]},{"cell_type":"markdown","metadata":{"id":"2tsckKDWw60c"},"source":["A partir de 50 caracteres empieza a ser mas habitual la cantidad de reviews de dicha longitud o mas"]},{"cell_type":"markdown","metadata":{"id":"CZgJREKsw60c"},"source":["# Limpieza de datos - Text normalizer\n","Empiezo usando el dataset de prueba (df_prueba), que contiene 100 comentarios.\n","\n","Despues, hay correr lo mismo pero con el dataset completo (df)"]},{"cell_type":"markdown","metadata":{"id":"hvIqUBcZw60d"},"source":[" * Remocion de caracteres acentuados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYORtEkKw60d"},"outputs":[],"source":["# Funcion para remover caracteres acentuados\n","def remove_accented_chars(text):\n","    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return new_text"]},{"cell_type":"markdown","metadata":{"id":"YeRiiBW-w60e"},"source":["* Expansion de contracciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QR9oPkKw60e"},"outputs":[],"source":["# Importo un dict de contracciones \n","# https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py\n","CONTRACTION_MAP = {\n","\"ain't\": \"is not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he'll've\": \"he he will have\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'd'y\": \"how do you\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how is\",\n","\"I'd\": \"I would\",\n","\"I'd've\": \"I would have\",\n","\"I'll\": \"I will\",\n","\"I'll've\": \"I will have\",\n","\"I'm\": \"I am\",\n","\"I've\": \"I have\",\n","\"i'd\": \"i would\",\n","\"i'd've\": \"i would have\",\n","\"i'll\": \"i will\",\n","\"i'll've\": \"i will have\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"it'd've\": \"it would have\",\n","\"it'll\": \"it will\",\n","\"it'll've\": \"it will have\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"mightn't've\": \"might not have\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"mustn't've\": \"must not have\",\n","\"needn't\": \"need not\",\n","\"needn't've\": \"need not have\",\n","\"o'clock\": \"of the clock\",\n","\"oughtn't\": \"ought not\",\n","\"oughtn't've\": \"ought not have\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"shan't've\": \"shall not have\",\n","\"she'd\": \"she would\",\n","\"she'd've\": \"she would have\",\n","\"she'll\": \"she will\",\n","\"she'll've\": \"she will have\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"shouldn't've\": \"should not have\",\n","\"so've\": \"so have\",\n","\"so's\": \"so as\",\n","\"that'd\": \"that would\",\n","\"that'd've\": \"that would have\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there would\",\n","\"there'd've\": \"there would have\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'd've\": \"they would have\",\n","\"they'll\": \"they will\",\n","\"they'll've\": \"they will have\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"to've\": \"to have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\n","\"we'd've\": \"we would have\",\n","\"we'll\": \"we will\",\n","\"we'll've\": \"we will have\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\n","\"what'll've\": \"what will have\",\n","\"what're\": \"what are\",\n","\"what's\": \"what is\",\n","\"what've\": \"what have\",\n","\"when's\": \"when is\",\n","\"when've\": \"when have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where is\",\n","\"where've\": \"where have\",\n","\"who'll\": \"who will\",\n","\"who'll've\": \"who will have\",\n","\"who's\": \"who is\",\n","\"who've\": \"who have\",\n","\"why's\": \"why is\",\n","\"why've\": \"why have\",\n","\"will've\": \"will have\",\n","\"won't\": \"will not\",\n","\"won't've\": \"will not have\",\n","\"would've\": \"would have\",\n","\"wouldn't\": \"would not\",\n","\"wouldn't've\": \"would not have\",\n","\"y'all\": \"you all\",\n","\"y'all'd\": \"you all would\",\n","\"y'all'd've\": \"you all would have\",\n","\"y'all're\": \"you all are\",\n","\"y'all've\": \"you all have\",\n","\"you'd\": \"you would\",\n","\"you'd've\": \"you would have\",\n","\"you'll\": \"you will\",\n","\"you'll've\": \"you will have\",\n","\"you're\": \"you are\",\n","\"you've\": \"you have\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKbu5PM0w60g"},"outputs":[],"source":["import re \n","def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n","    \n","    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n","                                      flags=re.IGNORECASE|re.DOTALL)\n","    def expand_match(contraction):\n","        match = contraction.group(0)\n","        first_char = match[0]\n","        expanded_contraction = contraction_mapping.get(match)\\\n","                                if contraction_mapping.get(match)\\\n","                                else contraction_mapping.get(match.lower())                       \n","        expanded_contraction = first_char+expanded_contraction[1:]\n","        return expanded_contraction\n","        \n","    expanded_text = contractions_pattern.sub(expand_match, text)\n","    expanded_text = re.sub(\"'\", \"\", expanded_text)\n","    return expanded_text"]},{"cell_type":"markdown","metadata":{"id":"rc0r_zh3w60h"},"source":["* Remocion de caracteres especiales\n","\n","Aca saco tambien puntuaciones, parentesis, etc.\n","Capaz esto deberia hacerlo despues de tokenizar si no quiero perder esta informacion al hacer part of speach, etc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dp_cLTlrw60h"},"outputs":[],"source":["def remove_special_characters(text, remove_digits=False):\n","    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n","    text = re.sub(pattern, '', text)\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"wklk6oPcw60i"},"source":["* Lematizacion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2WV0yJkw60i"},"outputs":[],"source":["#nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUS_3A6yw60j"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer \n","def lemmatize_text(text):\n","    lemmatizer = WordNetLemmatizer() \n","    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"vSuM_jJ3w60j"},"source":["* Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GO9uBK5Xw60j"},"outputs":[],"source":["def simple_stemmer(text):\n","    ps = nltk.porter.PorterStemmer()\n","    text = ' '.join([ps.stem(word) for word in text.split()])\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"Bt1kA3x2w60k"},"source":["* Remocion de stopwrods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtgoJc0cw60k"},"outputs":[],"source":["# Importo tokenizador\n","tokenizer = ToktokTokenizer()\n","\n","# Importo lista de stopwords\n","stopword_list = nltk.corpus.stopwords.words('english')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xqE-vlsw60l"},"outputs":[],"source":["# Aclaro que palabras no quiero que esten entre las stopwords --> pronombres\n","exclude_from_stopwords = ['he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n","                         'herself', 'they', 'them', 'their', 'theirs', 'themselves']\n","for word in exclude_from_stopwords:\n","    stopword_list.remove(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFtQAGMpw60l"},"outputs":[],"source":["# Creo funcion que remueve stopwords\n","def remove_stopwords(text, is_lower_case=False):\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        filtered_tokens = [token for token in tokens if token not in stopword_list]\n","    else:\n","        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","    filtered_text = ' '.join(filtered_tokens)    \n","    return filtered_text"]},{"cell_type":"markdown","metadata":{"id":"v09-mpFqw60m"},"source":["* Normalizacion el txt de las reviews segun el tipo de ingles  (US o British)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--ZBoVGrw60m"},"outputs":[],"source":["def replace_all(text, mydict):\n","    for gb, us in mydict.items():\n","        text = text.replace(us, gb)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wq977-OLw60n"},"outputs":[],"source":["# Leo el archivo txt con el diccionario US --> British\n","with open(\"C:/Users/tomas/Desktop/Maestria Data Mining/Text Mining/us2gb-dictionary.txt\") as f:\n","  us2gb = dict(x.rstrip().split(None, 1) for x in f)"]},{"cell_type":"markdown","metadata":{"id":"p2hS2TD7w60n"},"source":["* Convertir emojis a palabras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnlZ9h9Iw60o"},"outputs":[],"source":["def extract_emoji(text):\n","    emoji.demojize(text, delimiters=(\"\", \"\"))\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"YtoboLlAw60o"},"source":["# Preprocesamiento de los reviews"]},{"cell_type":"markdown","metadata":{"id":"bLs-sNptw60p"},"source":["Todas las pruebas sobre df_prueba. Luego reemplazar y correrlo sobre el df general\n","\n","Creo la funcion \"preprocesamiento\" que hace una limpieza general de las reviews.\n","\n","Ademas, las tokeniza por oraciones y luego por palabras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8maInnUAw60p"},"outputs":[],"source":["# Creo la funcion preprocesamiento a ver si me devuelve lo que quiero\n","def preprocesamiento(review, remove_typos=True, accented_char_removal=True,\n","                     contraction_expansion=True, text_lower_case=True,\n","                     text_stem = False, text_lemmatization=False,\n","                     special_char_removal=True, stopword_removal=True,\n","                     english_norm = True, emoji_extraction = True):\n","    \n","    ###### LIMPIEZA ######\n","    # Corrije errores de ortografia\n","    if remove_typos:\n","        review = str(TextBlob(review).correct())\n","    # Normalizacion del ingles (lleva todo a British) \n","    if english_norm:\n","        review = replace_all(review, us2gb)\n","    # Convierte emojis a text\n","    if emoji_extraction:\n","        review = extract_emoji(review)\n","    # Remueve saltos de linea\n","    review = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',review)\n","    # Remueve acentos de los caracteres\n","    if accented_char_removal:\n","        review = remove_accented_chars(review)\n","    # Exande las contracciones. Ej: don't --> do not \n","    if contraction_expansion: \n","        review = expand_contractions(review)\n","    # Convierte todo a minuscula\n","    if text_lower_case:    \n","        review = review.lower()\n","    # Remueve caracteres especiales\n","    if special_char_removal:\n","        # insert spaces between special characters to isolate them    \n","        special_char_pattern = re.compile(r'([{.(-)!}])')\n","        review = special_char_pattern.sub(\" \\\\1 \", review) \n","    # Remueve espacios en blanco\n","    review = re.sub(' +', ' ', review)\n","    # Remueve stopwords\n","    if stopword_removal:\n","        review = remove_stopwords(review, is_lower_case=True)\n","    # Lematizacion\n","    if text_lemmatization:\n","        review = lemmatize_text(review)\n","    # Stemming\n","    if text_stem:\n","        review = simple_stemmer(review)\n","        \n","    ###### TOKENIZACION ######        \n","    # Tokeniza por oracion\n","    oraciones_tokenizadas = sent_tokenize(review)\n","    # Crea lista vacia de oraciones\n","    oraciones = []      \n","        # Agrega las palabras tokenizadas a una lista de oracion.\n","        # Cada lista de oracion se agrega a una lista de oraciones.\n","    for oracion in oraciones_tokenizadas:\n","        oraciones.append([word for word in word_tokenize(oracion) if word.isalpha()])\n","    # Deuvelve la lista de oraciones\n","    return (oraciones)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFYalagfw60q"},"outputs":[],"source":["# Pruebo la funcion \"preprocesamiento\" con un apply lambda\n","df['rev_token']  = df['rev_content'].apply(lambda x: preprocesamiento(x, stopword_removal=False ))"]},{"cell_type":"markdown","metadata":{"id":"jA9ycZ5gw60r"},"source":["## Normalizaciones adicionales\n","\n","libreria \"normalise\"\n","https://github.com/EFord36/normalise\n","\n","Ejemplos de tokens a normalizar incluyen:\n","-    numeros --> porcentajes, fechas, dinero\n","-    abreviaciones y acronimos\n","-    hashtags y sitios webs\n","-    errores de ortografia (double cheking, teniendo en cuenta que previamente el texto de las reviews ya fue limpiado de typos en la funcion \"preprocesamiento\")\n"]},{"cell_type":"markdown","metadata":{"id":"feZwwgFyw60s"},"source":["Defino una funcion de normalizacion\n","1) Creo la funcion\n","2) input: review --> lista de listas donde cada lista es una oracion\n","3) creo una lista vacia --> review_output\n","4) para cada oracion dentro de la review:\n","    a) normalizo\n","    b) appendeo a la lista review_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2WLoocIw60s"},"outputs":[],"source":["# Defino una funcion que normalice cada oracion del review\n","def normalizacion(review):\n","    # input: review --> lista de listas donde cada lista es una oracion\n","    review_output = []\n","    for oracion in review:\n","        # Normalizo cada oracion y la appendeo a una lista\n","       review_output.append(normalise(oracion, verbose=False))\n","    # Devuelvo la lista de oraciones normalizadas\n","    return (review_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCuVcughw60t"},"outputs":[],"source":["# Aplico la funcion de normalizacion a la columna de reviews tokenizados\n","df['rev_norm_token']  = df['rev_token'].apply(lambda x: normalizacion(x))"]},{"cell_type":"markdown","metadata":{"id":"QQGtP4Jvw60t"},"source":["### Armar 'collocations'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fg7KByxWw60t"},"outputs":[],"source":["# Armo el corpus completo para collocations\n","corpus_collocations = []\n","for rev in df.rev_content:\n","  rev = rev.lower()\n","  #split by sentence\n","  sentences = sent_tokenize(rev)\n","  for sent in sentences:\n","    # word tokenize and append the sentence as a list of words\n","    corpus_collocations.append([word for word in word_tokenize(sent) if word.isalpha()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEB_OWxdw60u"},"outputs":[],"source":["collocations = Phrases(sentences=corpus_collocations , min_count=10,threshold=0.5,scoring='npmi') # threshold: minimo score aceptado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpFfYB0Qw60u"},"outputs":[],"source":["to_collocations = Phraser(collocations)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNV3YLa8w60v","outputId":"ed890eff-4e9b-44c3-99cf-346f84e3c1d4"},"outputs":[{"data":{"text/plain":["(154, 2)"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["df_collocations =pd.DataFrame([x for x in collocations.export_phrases(corpus_collocations)],columns=[\"bigram\",\"score\"])\n","df_collocations.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0tV1m7Fw60v","outputId":"aadb3c24-0545-4eee-cc52-6d3e2fea05b5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bigram</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>11</td>\n","      <td>b'mental health'</td>\n","      <td>0.935302</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>b'the book'</td>\n","      <td>0.595617</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>b'if you'</td>\n","      <td>0.586535</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>b'this film'</td>\n","      <td>0.549967</td>\n","    </tr>\n","    <tr>\n","      <td>0</td>\n","      <td>b'to see'</td>\n","      <td>0.510140</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>b'this movie'</td>\n","      <td>0.508722</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>b'to be'</td>\n","      <td>0.505075</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              bigram     score\n","11  b'mental health'  0.935302\n","1        b'the book'  0.595617\n","5          b'if you'  0.586535\n","18      b'this film'  0.549967\n","0          b'to see'  0.510140\n","3      b'this movie'  0.508722\n","8           b'to be'  0.505075"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["df_collocations.drop_duplicates().sort_values(by=\"score\",ascending=False).head(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33_IR1p4w60w","outputId":"f1096d99-2079-436a-fe52-50875ee1fbfa"},"outputs":[{"data":{"text/plain":["<gensim.interfaces.TransformedCorpus at 0x2ab12b701c8>"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["df['rev_token_ngrams']  = df['rev_token'].apply(lambda x: to_collocations[x])\n","df.loc[0,'rev_token_ngrams']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4W9tVSYHw60x","outputId":"b9d66d2e-0dba-4646-cb4e-8f0b339bd644"},"outputs":[{"name":"stdout","output_type":"stream","text":["el corpus de imdb tiene 51 reviews, 470 oraciones y 6515 palabras\n"]}],"source":["print(\"el corpus de imdb tiene\", len(df['rev_token']), \"reviews,\",sum([len(x) for x in df['rev_token']]),\"oraciones y\", sum([len(y) for x in df['rev_token'] for y in x]) ,\"palabras\")"]},{"cell_type":"markdown","metadata":{"id":"Eirk3OoYw60y"},"source":["# BORRADOR:\n","## Tokenizacion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zEfqy_bBw60y"},"outputs":[],"source":["import nltk\n","#nltk.download('punkt')\n","from nltk import word_tokenize\n","from nltk import sent_tokenize\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihV5Zym3w60z"},"outputs":[],"source":["def custom_tokenize(text):\n","    if not text:\n","        print('The text to be tokenized is a None type. Defaulting to blank string.')\n","        text = ''\n","    # find a word character: character from a-z, A-Z, 0-9\n","    return nltk.RegexpTokenizer(r'\\w+').tokenize(text)\n","df_prueba['tokenized_rev'] = df_prueba.rev_content.apply(custom_tokenize)\n","df_prueba.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmr4n5k6w600"},"outputs":[],"source":["tokens_prueba = df_prueba.loc[41,'tokenized_rev']\n","tokens_prueba"]},{"cell_type":"markdown","metadata":{"id":"8kLGo_EUw600"},"source":["## Funcion normalizadora de texto\n","Reune el resto de las funciones de normalizacion construidas previamente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tR1bdHrJw600"},"outputs":[],"source":["def normalize_corpus(corpus,contraction_expansion=True,\n","                     accented_char_removal=True, text_lower_case=True, text_stem = False,\n","                     text_lemmatization=False, special_char_removal=True, \n","                     stopword_removal=True, remove_digits=True):\n","    \n","    normalized_corpus = []\n","    # normalize each document in the corpus\n","    for doc in corpus:\n","        # remove accented characters\n","        if accented_char_removal:\n","            doc = remove_accented_chars(doc)\n","        # expand contractions    \n","        if contraction_expansion:\n","            doc = expand_contractions(doc)\n","        # lowercase the text    \n","        if text_lower_case:\n","            doc = doc.lower()\n","        # remove extra newlines\n","        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n","        # stem text\n","        if text_stem:\n","            doc = simple_stemmer(doc)\n","        # lemmatize text\n","        if text_lemmatization:\n","            doc = lemmatize_text(doc)\n","        # remove special characters and\\or digits    \n","        if special_char_removal:\n","            # insert spaces between special characters to isolate them    \n","            special_char_pattern = re.compile(r'([{.(-)!}])')\n","            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n","            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n","        # remove extra whitespace\n","        doc = re.sub(' +', ' ', doc)\n","        # remove stopwords\n","        if stopword_removal:\n","            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n","            \n","        normalized_corpus.append(doc)\n","        \n","    return normalized_corpus"]},{"cell_type":"markdown","metadata":{"id":"rgsKxNZDw601"},"source":["## Prueba de preprocesamiento y normalizacion de reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URmq6Y0Yw602","outputId":"bf172d0c-cbc8-4199-9444-ac08e4190416"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\tomas\\Anconda\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>imdbid</th>\n","      <th>rev_title</th>\n","      <th>rev_content</th>\n","      <th>rating</th>\n","      <th>rev_link</th>\n","      <th>rev_username</th>\n","      <th>rev_date</th>\n","      <th>clean_rev_content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>tt3907584</td>\n","      <td>A spectacular movie\\n</td>\n","      <td>The first act felt annoyingly pretentious, how...</td>\n","      <td>9</td>\n","      <td>/review/rw5556178/</td>\n","      <td>MR_Heraclius</td>\n","      <td>20200316</td>\n","      <td>first act felt annoyingly pretentious however ...</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>tt3907584</td>\n","      <td>so....read this up.\\n</td>\n","      <td>I did not know this was made after a book. I d...</td>\n","      <td>9</td>\n","      <td>/review/rw5515902/</td>\n","      <td>suciulaurentiucristian</td>\n","      <td>20200228</td>\n","      <td>know wa made book care book wa better actual c...</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>tt3907584</td>\n","      <td>they had so much but they gave too little\\n</td>\n","      <td>I had this feeling since they announced the mo...</td>\n","      <td>5</td>\n","      <td>/review/rw5515384/</td>\n","      <td>mermariam</td>\n","      <td>20200228</td>\n","      <td>feeling since they announced movie disappointe...</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>tt3907584</td>\n","      <td>People who call him selfish have never experi...</td>\n","      <td>I saw some negative reviews not getting the lo...</td>\n","      <td>10</td>\n","      <td>/review/rw5525473/</td>\n","      <td>viktoriabr-21711</td>\n","      <td>20200304</td>\n","      <td>saw negative review getting logic him comittin...</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>tt3907584</td>\n","      <td>Beautiful movie.\\n</td>\n","      <td>Randomly found this movie today and I only hav...</td>\n","      <td>10</td>\n","      <td>/review/rw5516559/</td>\n","      <td>aeo-90520</td>\n","      <td>20200229</td>\n","      <td>randomly found movie today praise acting wonde...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0     imdbid                                          rev_title  \\\n","0           0  tt3907584                              A spectacular movie\\n   \n","1           1  tt3907584                              so....read this up.\\n   \n","2           2  tt3907584        they had so much but they gave too little\\n   \n","3           3  tt3907584   People who call him selfish have never experi...   \n","4           4  tt3907584                                 Beautiful movie.\\n   \n","\n","                                         rev_content  rating  \\\n","0  The first act felt annoyingly pretentious, how...       9   \n","1  I did not know this was made after a book. I d...       9   \n","2  I had this feeling since they announced the mo...       5   \n","3  I saw some negative reviews not getting the lo...      10   \n","4  Randomly found this movie today and I only hav...      10   \n","\n","             rev_link            rev_username  rev_date  \\\n","0  /review/rw5556178/            MR_Heraclius  20200316   \n","1  /review/rw5515902/  suciulaurentiucristian  20200228   \n","2  /review/rw5515384/               mermariam  20200228   \n","3  /review/rw5525473/        viktoriabr-21711  20200304   \n","4  /review/rw5516559/               aeo-90520  20200229   \n","\n","                                   clean_rev_content  \n","0  first act felt annoyingly pretentious however ...  \n","1  know wa made book care book wa better actual c...  \n","2  feeling since they announced movie disappointe...  \n","3  saw negative review getting logic him comittin...  \n","4  randomly found movie today praise acting wonde...  "]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["df_prueba['clean_rev_content'] = normalize_corpus(df_prueba['rev_content'])\n","df_prueba.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRPmuryyw602"},"outputs":[],"source":["#df_prueba_corpus = list(df_prueba['clean_rev_content'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F64DaY88w603","outputId":"40976bf1-498f-4f3e-d22f-8a74ef7aeddf"},"outputs":[{"data":{"text/plain":["{'rev_content': \"I did not know this was made after a book. I don't care if the book was better. The actual capacity of a screenplay writer and a director to reproduce the book can and will be forever disputed.\\nWhat i want to say is that this movie takes you place. You laugh. You care. You love and You cry. This movie has emotion and has life. You, for the time it runs, feel.Is not this the purpose of a movie? To make you feel? to connect to the characters - their joy, their sorrow?\\nThis aside - the acting ,the music, the photography....everything is so well suited to the scenario and the purpose of the movie. To feel. Not to understand but to feel.\\nWatch it to live true emotions. Do not if you dont.\\nFor what was this movie made, it's a jewel. Take it as it is.\",\n"," 'clean_rev_content': 'know wa made book care book wa better actual capacity screenplay writer director reproduce book forever disputed want say movie take place laugh care love cry movie ha emotion ha life time runs feel purpose movie make feel connect character their joy their sorrow aside acting music photography everything well suited scenario purpose movie feel understand feel watch live true emotions dont wa movie made jewel take'}"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["df_prueba.iloc[1][['rev_content', 'clean_rev_content']].to_dict()"]},{"cell_type":"markdown","metadata":{"id":"3fAvOhmlw604"},"source":["# Links utiles  \n","    - Spelling correction --> http://norvig.com/spell-correct.html\n","    - Mejorar lematizacion con text blob --> https://textblob.readthedocs.io/en/dev/quickstart.html\n","    \n","    - https://github.com/vaishalilambe/NLP-Basics--Data-Cleaning-Pipeline/blob/master/NLPBasics.ipynb\n","    - https://medium.com/@dobko_m/nlp-text-data-cleaning-and-preprocessing-ea3ffe0406c1\n","    - https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n","    - https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a : Punctuation removal is better to be done after the tokenization step, doing it before might cause undesirable effects.\n","  "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"NLP_Limpieza_de_datos_imdb (1) (1).ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}